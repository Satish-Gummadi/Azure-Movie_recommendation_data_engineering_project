{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "449f7f31-b494-484d-a0bc-f0a9df8fe37a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### GETTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f50fd7f8-4969-40d6-ae09-0f64aee88e4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings_filename = \"dbfs:/mnt/Files/Validateddata/ratings.csv\"\n",
    "movies_filename = \"dbfs:/mnt/Files/Validateddata/movies.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f980968e-4799-46c8-b90f-ec8f60cc81e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/Files/Validateddata/genome-tags.csv</td><td>genome-tags.csv</td><td>18103</td><td>1681901135000</td></tr><tr><td>dbfs:/mnt/Files/Validateddata/movies.csv</td><td>movies.csv</td><td>3038099</td><td>1681901136000</td></tr><tr><td>dbfs:/mnt/Files/Validateddata/ratings.csv</td><td>ratings.csv</td><td>678260987</td><td>1681901140000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/Files/Validateddata/genome-tags.csv",
         "genome-tags.csv",
         18103,
         1681901135000
        ],
        [
         "dbfs:/mnt/Files/Validateddata/movies.csv",
         "movies.csv",
         3038099,
         1681901136000
        ],
        [
         "dbfs:/mnt/Files/Validateddata/ratings.csv",
         "ratings.csv",
         678260987,
         1681901140000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /mnt/Files/Validateddata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebed3663-2658-4040-8b29-824b4940e916",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### A Little analysis on the movies.csv\n",
    "We will create 2 dataframes for our analysis which will make the visualization with Databricks display function pretty straightforward\n",
    "1) movie_based_on_time - We will drop the genres here final schema will be (movie_id,name,Year)\n",
    "2) movie_based_on_genres - Final schema would look like (movie_id,name_with_year,one_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0906a42-b790-4986-a8cb-5491eda719b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# working only one movie.csv right now\n",
    "movie_with_genre_df_schema = StructType(\n",
    "    [StructField('ID',IntegerType()),\n",
    "    StructField('title',StringType()),\n",
    "    StructField('genres',StringType())]\n",
    ")\n",
    "\n",
    "movie_df_schema = StructType(\n",
    "    [StructField('ID',IntegerType()),\n",
    "    StructField('title',StringType())]\n",
    ") # in this we have just removed genre column. We will transform this schema to add year column in later section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a4c709a-0463-43b4-a425-0800543a644f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating the dataframes\n",
    "movies_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True,inferSchema=False).schema(movie_df_schema).load(movies_filename)\n",
    "movies_with_genres_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True,inferSchema=False).schema(movie_with_genre_df_schema).load(movies_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2bde5da-abbf-4757-89a3-43ecc7d2270f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Inspecting the dataframes before transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6febafc6-afb4-4327-91c6-5730de642310",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+\n|ID |title                   |\n+---+------------------------+\n|1  |Toy Story (1995)        |\n|2  |Jumanji (1995)          |\n|3  |Grumpier Old Men (1995) |\n|4  |Waiting to Exhale (1995)|\n+---+------------------------+\nonly showing top 4 rows\n\n+---+------------------------+-------------------------------------------+\n|ID |title                   |genres                                     |\n+---+------------------------+-------------------------------------------+\n|1  |Toy Story (1995)        |Adventure|Animation|Children|Comedy|Fantasy|\n|2  |Jumanji (1995)          |Adventure|Children|Fantasy                 |\n|3  |Grumpier Old Men (1995) |Comedy|Romance                             |\n|4  |Waiting to Exhale (1995)|Comedy|Drama|Romance                       |\n+---+------------------------+-------------------------------------------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "movies_df.show(4,truncate=False)  # we will also use this for collaborative filtering\n",
    "movies_with_genres_df.show(4,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750507a0-5add-4315-8327-67eca2dbc264",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# transformating the dataframes\n",
    "\n",
    "from pyspark.sql.functions import split, regexp_extract\n",
    "\n",
    "movies_with_year_df = movies_df.select('ID','title',regexp_extract('title',r'\\((\\d+)\\)',1).alias('year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01ea11f7-e455-48b5-b20e-8515fce91a37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------+----+\n|ID |title                   |year|\n+---+------------------------+----+\n|1  |Toy Story (1995)        |1995|\n|2  |Jumanji (1995)          |1995|\n|3  |Grumpier Old Men (1995) |1995|\n|4  |Waiting to Exhale (1995)|1995|\n+---+------------------------+----+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dataframes after transformation\n",
    "movies_with_year_df.show(4,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c759cd7-26dc-4976-911a-0c2eaf904188",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Now we will use inbuilt functionality of Databricks for some insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40454671-b46b-43ef-9c36-d273d348c687",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>year</th><th>count</th></tr></thead><tbody><tr><td>2015</td><td>2513</td></tr><tr><td>2016</td><td>2488</td></tr><tr><td>2014</td><td>2406</td></tr><tr><td>2017</td><td>2374</td></tr><tr><td>2013</td><td>2173</td></tr><tr><td>2018</td><td>2034</td></tr><tr><td>2012</td><td>1978</td></tr><tr><td>2011</td><td>1838</td></tr><tr><td>2009</td><td>1723</td></tr><tr><td>2010</td><td>1691</td></tr><tr><td>2008</td><td>1632</td></tr><tr><td>2007</td><td>1498</td></tr><tr><td>2006</td><td>1446</td></tr><tr><td>2005</td><td>1255</td></tr><tr><td>2004</td><td>1172</td></tr><tr><td>2003</td><td>1028</td></tr><tr><td>2002</td><td>1024</td></tr><tr><td>2019</td><td>994</td></tr><tr><td>2001</td><td>971</td></tr><tr><td>2000</td><td>929</td></tr><tr><td>1999</td><td>812</td></tr><tr><td>1998</td><td>803</td></tr><tr><td>1997</td><td>792</td></tr><tr><td>1995</td><td>705</td></tr><tr><td>1996</td><td>703</td></tr><tr><td>1994</td><td>644</td></tr><tr><td>1988</td><td>628</td></tr><tr><td>1989</td><td>609</td></tr><tr><td>1987</td><td>595</td></tr><tr><td>1993</td><td>571</td></tr><tr><td>1992</td><td>563</td></tr><tr><td>1972</td><td>556</td></tr><tr><td>1990</td><td>538</td></tr><tr><td>1971</td><td>534</td></tr><tr><td>1991</td><td>524</td></tr><tr><td>1973</td><td>515</td></tr><tr><td>1986</td><td>513</td></tr><tr><td>1985</td><td>513</td></tr><tr><td>1974</td><td>511</td></tr><tr><td>1970</td><td>491</td></tr><tr><td>1976</td><td>490</td></tr><tr><td>1979</td><td>487</td></tr><tr><td>1984</td><td>471</td></tr><tr><td>1980</td><td>470</td></tr><tr><td>1977</td><td>469</td></tr><tr><td>1981</td><td>462</td></tr><tr><td>1982</td><td>461</td></tr><tr><td>1983</td><td>460</td></tr><tr><td>1968</td><td>458</td></tr><tr><td>1978</td><td>457</td></tr><tr><td>1975</td><td>448</td></tr><tr><td>1969</td><td>442</td></tr><tr><td></td><td>410</td></tr><tr><td>1966</td><td>404</td></tr><tr><td>1967</td><td>388</td></tr><tr><td>1964</td><td>367</td></tr><tr><td>1965</td><td>362</td></tr><tr><td>1957</td><td>336</td></tr><tr><td>1963</td><td>317</td></tr><tr><td>1955</td><td>315</td></tr><tr><td>1958</td><td>309</td></tr><tr><td>1953</td><td>304</td></tr><tr><td>1959</td><td>304</td></tr><tr><td>1956</td><td>294</td></tr><tr><td>1962</td><td>289</td></tr><tr><td>1961</td><td>289</td></tr><tr><td>1952</td><td>287</td></tr><tr><td>1954</td><td>285</td></tr><tr><td>1950</td><td>279</td></tr><tr><td>1960</td><td>269</td></tr><tr><td>1951</td><td>268</td></tr><tr><td>1949</td><td>266</td></tr><tr><td>1942</td><td>257</td></tr><tr><td>1948</td><td>245</td></tr><tr><td>1937</td><td>235</td></tr><tr><td>1941</td><td>232</td></tr><tr><td>1947</td><td>232</td></tr><tr><td>1936</td><td>230</td></tr><tr><td>1935</td><td>227</td></tr><tr><td>1943</td><td>223</td></tr><tr><td>1940</td><td>223</td></tr><tr><td>1934</td><td>219</td></tr><tr><td>1945</td><td>219</td></tr><tr><td>1933</td><td>218</td></tr><tr><td>1946</td><td>217</td></tr><tr><td>1932</td><td>215</td></tr><tr><td>1939</td><td>214</td></tr><tr><td>1944</td><td>212</td></tr><tr><td>1938</td><td>209</td></tr><tr><td>1931</td><td>173</td></tr><tr><td>1930</td><td>129</td></tr><tr><td>1929</td><td>117</td></tr><tr><td>1928</td><td>77</td></tr><tr><td>1926</td><td>66</td></tr><tr><td>1927</td><td>62</td></tr><tr><td>1925</td><td>52</td></tr><tr><td>1921</td><td>48</td></tr><tr><td>1922</td><td>46</td></tr><tr><td>1924</td><td>42</td></tr><tr><td>1914</td><td>42</td></tr><tr><td>1916</td><td>37</td></tr><tr><td>1920</td><td>36</td></tr><tr><td>1919</td><td>30</td></tr><tr><td>1923</td><td>30</td></tr><tr><td>1915</td><td>28</td></tr><tr><td>1903</td><td>26</td></tr><tr><td>1917</td><td>26</td></tr><tr><td>1900</td><td>22</td></tr><tr><td>1896</td><td>22</td></tr><tr><td>1894</td><td>22</td></tr><tr><td>1918</td><td>20</td></tr><tr><td>1912</td><td>19</td></tr><tr><td>1907</td><td>18</td></tr><tr><td>1906</td><td>17</td></tr><tr><td>1908</td><td>17</td></tr><tr><td>1909</td><td>16</td></tr><tr><td>1895</td><td>15</td></tr><tr><td>1913</td><td>15</td></tr><tr><td>1898</td><td>14</td></tr><tr><td>1897</td><td>13</td></tr><tr><td>1901</td><td>12</td></tr><tr><td>1910</td><td>11</td></tr><tr><td>1911</td><td>11</td></tr><tr><td>1904</td><td>11</td></tr><tr><td>1905</td><td>10</td></tr><tr><td>1899</td><td>8</td></tr><tr><td>1891</td><td>6</td></tr><tr><td>1902</td><td>6</td></tr><tr><td>1890</td><td>5</td></tr><tr><td>1888</td><td>4</td></tr><tr><td>1892</td><td>3</td></tr><tr><td>1880</td><td>1</td></tr><tr><td>69</td><td>1</td></tr><tr><td>1874</td><td>1</td></tr><tr><td>1887</td><td>1</td></tr><tr><td>1883</td><td>1</td></tr><tr><td>1878</td><td>1</td></tr><tr><td>06</td><td>1</td></tr><tr><td>500</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2015",
         2513
        ],
        [
         "2016",
         2488
        ],
        [
         "2014",
         2406
        ],
        [
         "2017",
         2374
        ],
        [
         "2013",
         2173
        ],
        [
         "2018",
         2034
        ],
        [
         "2012",
         1978
        ],
        [
         "2011",
         1838
        ],
        [
         "2009",
         1723
        ],
        [
         "2010",
         1691
        ],
        [
         "2008",
         1632
        ],
        [
         "2007",
         1498
        ],
        [
         "2006",
         1446
        ],
        [
         "2005",
         1255
        ],
        [
         "2004",
         1172
        ],
        [
         "2003",
         1028
        ],
        [
         "2002",
         1024
        ],
        [
         "2019",
         994
        ],
        [
         "2001",
         971
        ],
        [
         "2000",
         929
        ],
        [
         "1999",
         812
        ],
        [
         "1998",
         803
        ],
        [
         "1997",
         792
        ],
        [
         "1995",
         705
        ],
        [
         "1996",
         703
        ],
        [
         "1994",
         644
        ],
        [
         "1988",
         628
        ],
        [
         "1989",
         609
        ],
        [
         "1987",
         595
        ],
        [
         "1993",
         571
        ],
        [
         "1992",
         563
        ],
        [
         "1972",
         556
        ],
        [
         "1990",
         538
        ],
        [
         "1971",
         534
        ],
        [
         "1991",
         524
        ],
        [
         "1973",
         515
        ],
        [
         "1986",
         513
        ],
        [
         "1985",
         513
        ],
        [
         "1974",
         511
        ],
        [
         "1970",
         491
        ],
        [
         "1976",
         490
        ],
        [
         "1979",
         487
        ],
        [
         "1984",
         471
        ],
        [
         "1980",
         470
        ],
        [
         "1977",
         469
        ],
        [
         "1981",
         462
        ],
        [
         "1982",
         461
        ],
        [
         "1983",
         460
        ],
        [
         "1968",
         458
        ],
        [
         "1978",
         457
        ],
        [
         "1975",
         448
        ],
        [
         "1969",
         442
        ],
        [
         "",
         410
        ],
        [
         "1966",
         404
        ],
        [
         "1967",
         388
        ],
        [
         "1964",
         367
        ],
        [
         "1965",
         362
        ],
        [
         "1957",
         336
        ],
        [
         "1963",
         317
        ],
        [
         "1955",
         315
        ],
        [
         "1958",
         309
        ],
        [
         "1953",
         304
        ],
        [
         "1959",
         304
        ],
        [
         "1956",
         294
        ],
        [
         "1962",
         289
        ],
        [
         "1961",
         289
        ],
        [
         "1952",
         287
        ],
        [
         "1954",
         285
        ],
        [
         "1950",
         279
        ],
        [
         "1960",
         269
        ],
        [
         "1951",
         268
        ],
        [
         "1949",
         266
        ],
        [
         "1942",
         257
        ],
        [
         "1948",
         245
        ],
        [
         "1937",
         235
        ],
        [
         "1941",
         232
        ],
        [
         "1947",
         232
        ],
        [
         "1936",
         230
        ],
        [
         "1935",
         227
        ],
        [
         "1943",
         223
        ],
        [
         "1940",
         223
        ],
        [
         "1934",
         219
        ],
        [
         "1945",
         219
        ],
        [
         "1933",
         218
        ],
        [
         "1946",
         217
        ],
        [
         "1932",
         215
        ],
        [
         "1939",
         214
        ],
        [
         "1944",
         212
        ],
        [
         "1938",
         209
        ],
        [
         "1931",
         173
        ],
        [
         "1930",
         129
        ],
        [
         "1929",
         117
        ],
        [
         "1928",
         77
        ],
        [
         "1926",
         66
        ],
        [
         "1927",
         62
        ],
        [
         "1925",
         52
        ],
        [
         "1921",
         48
        ],
        [
         "1922",
         46
        ],
        [
         "1924",
         42
        ],
        [
         "1914",
         42
        ],
        [
         "1916",
         37
        ],
        [
         "1920",
         36
        ],
        [
         "1919",
         30
        ],
        [
         "1923",
         30
        ],
        [
         "1915",
         28
        ],
        [
         "1903",
         26
        ],
        [
         "1917",
         26
        ],
        [
         "1900",
         22
        ],
        [
         "1896",
         22
        ],
        [
         "1894",
         22
        ],
        [
         "1918",
         20
        ],
        [
         "1912",
         19
        ],
        [
         "1907",
         18
        ],
        [
         "1906",
         17
        ],
        [
         "1908",
         17
        ],
        [
         "1909",
         16
        ],
        [
         "1895",
         15
        ],
        [
         "1913",
         15
        ],
        [
         "1898",
         14
        ],
        [
         "1897",
         13
        ],
        [
         "1901",
         12
        ],
        [
         "1910",
         11
        ],
        [
         "1911",
         11
        ],
        [
         "1904",
         11
        ],
        [
         "1905",
         10
        ],
        [
         "1899",
         8
        ],
        [
         "1891",
         6
        ],
        [
         "1902",
         6
        ],
        [
         "1890",
         5
        ],
        [
         "1888",
         4
        ],
        [
         "1892",
         3
        ],
        [
         "1880",
         1
        ],
        [
         "69",
         1
        ],
        [
         "1874",
         1
        ],
        [
         "1887",
         1
        ],
        [
         "1883",
         1
        ],
        [
         "1878",
         1
        ],
        [
         "06",
         1
        ],
        [
         "500",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from here we can look at the cound and find the maximum number of movies produced in year 2009\n",
    "display(movies_with_year_df.groupBy('year').count().orderBy('count',ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "003254ed-7cb6-4e89-9a3b-6ec280fb1f92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Lets check the Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d1442f-98b8-452b-a981-63e49817f017",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to avoid the action we are explicitly defining the schema\n",
    "ratings_df_schema = StructType(\n",
    "[StructField('userId',IntegerType()),\n",
    "StructField('movieId',IntegerType()),\n",
    "StructField('rating',DoubleType())])  \n",
    "# we have dropped the timestamp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b647bc0d-bfb5-4c5a-9664-83b941dfe47b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n|userId|movieId|rating|\n+------+-------+------+\n|     1|    296|   5.0|\n|     1|    306|   3.5|\n|     1|    307|   5.0|\n|     1|    665|   5.0|\n+------+-------+------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "# creating the df\n",
    "ratings_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True,inferSchema=False).schema(ratings_df_schema).load(ratings_filename)\n",
    "\n",
    "ratings_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1efa1951-6dab-4b59-bc1d-aa4ed37b38c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Caching the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189d5f13-70dd-419b-b3ec-e400cdc22b18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: DataFrame[userId: int, movieId: int, rating: double]"
     ]
    }
   ],
   "source": [
    "# We will cache both the dataframes now\n",
    "\n",
    "movies_df.cache()\n",
    "ratings_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2bf43a7-590c-4ec3-b7df-be30e23f7b53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Global Popularity\n",
    "Here we will put a constraint on the no. of reviews given by discarding the movies where the count of ratings is less than 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa18cef0-60da-4e67-8dc4-b00f3eea4033",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_ids_with_avg_ratings_df:\n+-------+-----+------------------+\n|movieId|count|average           |\n+-------+-----+------------------+\n|1088   |11935|3.25002094679514  |\n|1580   |40308|3.5817083457378187|\n|3175   |14659|3.6077836141619484|\n|44022  |4833 |3.2593627146699773|\n+-------+-----+------------------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# from ratings dataframe we will create a movie_ids_with_avg_ratings_df that combines the two DataFrames\n",
    "movie_ids_with_avg_ratings_df = ratings_df.groupBy('movieId').agg(F.count(ratings_df.rating).alias('count'),F.avg(ratings_df.rating).alias('average'))\n",
    "print('movie_ids_with_avg_ratings_df:')\n",
    "movie_ids_with_avg_ratings_df.show(4,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4c669a-6528-4542-bc1b-8aaa36cf98ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+--------------------------------+\n|movieId|count|average           |title                           |\n+-------+-----+------------------+--------------------------------+\n|1088   |11935|3.25002094679514  |Dirty Dancing (1987)            |\n|1580   |40308|3.5817083457378187|Men in Black (a.k.a. MIB) (1997)|\n|3175   |14659|3.6077836141619484|Galaxy Quest (1999)             |\n|44022  |4833 |3.2593627146699773|Ice Age 2: The Meltdown (2006)  |\n+-------+-----+------------------+--------------------------------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "# this dataframe will have movies name with movieid for better understanding\n",
    "movie_names_with_avg_ratings_df = movie_ids_with_avg_ratings_df.join(movies_df,F.col('movieId')==F.col('ID')).drop('ID')\n",
    "movie_names_with_avg_ratings_df.show(4,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79986f07-f22a-45d0-8c6f-51a4638ea887",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+---------------------------------------------------------------------------+\n|movieId|count|average           |title                                                                      |\n+-------+-----+------------------+---------------------------------------------------------------------------+\n|171011 |1124 |4.483096085409253 |Planet Earth II (2016)                                                     |\n|159817 |1747 |4.464796794504865 |Planet Earth (2006)                                                        |\n|318    |81482|4.413576004516335 |Shawshank Redemption, The (1994)                                           |\n|170705 |1356 |4.398598820058997 |Band of Brothers (2001)                                                    |\n|858    |52498|4.324336165187245 |Godfather, The (1972)                                                      |\n|179135 |659  |4.289833080424886 |Blue Planet II (2017)                                                      |\n|50     |55366|4.284353213163313 |Usual Suspects, The (1995)                                                 |\n|1221   |34188|4.2617585117585115|Godfather: Part II, The (1974)                                             |\n|163809 |546  |4.258241758241758 |Over the Garden Wall (2013)                                                |\n|2019   |13367|4.25476920775043  |Seven Samurai (Shichinin no samurai) (1954)                                |\n|142115 |564  |4.24822695035461  |The Blue Planet (2001)                                                     |\n|527    |60411|4.247579083279535 |Schindler's List (1993)                                                    |\n|1203   |16569|4.243014062405697 |12 Angry Men (1957)                                                        |\n|904    |20162|4.237947624243627 |Rear Window (1954)                                                         |\n|2959   |58773|4.228310618821568 |Fight Club (1999)                                                          |\n|1193   |36058|4.2186616007543405|One Flew Over the Cuckoo's Nest (1975)                                     |\n|750    |26714|4.215804447106386 |Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)|\n|5618   |22719|4.212267265284564 |Spirited Away (Sen to Chihiro no kamikakushi) (2001)                       |\n|166024 |1030 |4.210194174757282 |Whiplash (2013)                                                            |\n|912    |26890|4.206563778356267 |Casablanca (1942)                                                          |\n+-------+-----+------------------+---------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# so let us see the global popularity\n",
    "movies_with_500_ratings_or_more = movie_names_with_avg_ratings_df.filter(movie_names_with_avg_ratings_df['count'] >= 500).orderBy('average',ascending=False)\n",
    "movies_with_500_ratings_or_more.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdf122f9-0815-4380-871d-33151653331e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A good thing to observe here is that it has a lot of simalarity with IMDB top 250 movies\n",
    "If there is a cold start problem(new user), we can just recommend the global populars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a4c2588-b0b3-4f48-b27c-2bbe5af61cdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Collaborative Filtering \n",
    "We will use the Matrix Factorization algorithm present in spark MLib called as ALS quora explination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5016cd84-6516-4d58-9d71-dc15b9ad3ee5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Splitting in Train, Test and Validation Dataset\n",
    "As with all the machine learning algorithms in practice we have to tune the parameters and then test accuracy. For this we will split the data into 3 parts Train, Test(checking the final accuracy) and Validation(optimizing the hyperparameters) data. For more information about this refer brillant lecture by Nando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0abbb469-7a46-487a-95f6-7c68d4bff9ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 14999112, validation: 4999908, test: 5001075\n\n+------+-------+------+\n|userId|movieId|rating|\n+------+-------+------+\n|1     |306    |3.5   |\n|1     |307    |5.0   |\n|1     |665    |5.0   |\n|1     |899    |3.5   |\n+------+-------+------+\nonly showing top 4 rows\n\n+------+-------+------+\n|userId|movieId|rating|\n+------+-------+------+\n|1     |1250   |4.0   |\n|1     |2011   |2.5   |\n|1     |2161   |3.5   |\n|1     |2351   |4.5   |\n+------+-------+------+\nonly showing top 4 rows\n\n+------+-------+------+\n|userId|movieId|rating|\n+------+-------+------+\n|1     |296    |5.0   |\n|1     |1217   |3.5   |\n|1     |2068   |2.5   |\n|1     |2843   |4.5   |\n+------+-------+------+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "# We will hold 60% for training, 20% of our data for validation and leave 20% of data for testing\n",
    "\n",
    "seed = 4\n",
    "(split_60_df,split_a_20_df,split_b_20_df) = ratings_df.randomSplit([0.6,0.2,0.2],seed)\n",
    "\n",
    "# Lets cache these datasets for performance\n",
    "training_df = split_60_df.cache()\n",
    "validation_df = split_a_20_df.cache()\n",
    "test_df = split_b_20_df.cache()\n",
    "\n",
    "print('Training: {0}, validation: {1}, test: {2}\\n'.format(training_df.count(),validation_df.count(),test_df.count()))\n",
    "\n",
    "training_df.show(4,truncate=False)\n",
    "validation_df.show(4,truncate=False)\n",
    "test_df.show(4,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa216404-491f-4063-9253-aaf2c5e58d47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From above we can see 10 million training samples, 4 million validation samples and 4 million test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f22bf65e-0ba3-44a5-89af-0d6ac50f56aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Alternating Least Square (ALS)\n",
    "\n",
    "Need of cross validation, some problems and solutions here I am copying it directly from the Assignment notebook\n",
    "\n",
    "A challenge for collaborative filtering is how to provide ratings to new users( a user who has not provided any ratings at all). Some recommendations systems choose to provide new users with a set of default ratings (e.g., an average value across all ratings, while others choose to provide no ratings for new users. Spark's ALS algorithm yields a NaN value when asked to provide a rating for a new user\n",
    "\n",
    "Using the ML Pipeline's CrossValidator with ALS is thus problematic, because cross validation involves dividing the training data into a set of folds(e.g., three sets) and then using those folds for testing and evaluating the parameters during the parameter grid search process. It is likely that some of the folds will contain users that are not in the other folds, and, as a result ALS produces NaN values for those new users. When the CrossValidator uses the Evaluator (RMSE) to compute an error metric, the RMSE algorithm will return NaN. This will make all of the parameters in the parameter grid appear to be equally good(or bad).\n",
    "\n",
    "You can read the discussion on Spark JIRA 14489 about this issue. There are proposed workarounds of having ALS provide default values or having RMSE drop NaN values. Both introduce potential issues. We have chose to have RMSE drop NaN values. While this does not solve the underlying issue of ALS not predicting a value for a new user,it does provide some evaluation value. We manually implement the parameter grid search process using a for loop(below) and remove the NaN values before using RMSE\n",
    "\n",
    "For a production application, you would want to consider the tradeoffs in how to handle new users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5af99e02-15a6-464b-951c-f71436619efd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# # our ALS learner\n",
    "# als = ALS()\n",
    "\n",
    "# # now we set the parameters for the method\n",
    "# als.setMaxIter(5)\\\n",
    "#    .setSeed(seed)\\\n",
    "#    .setRegParam(0.1)\\\n",
    "#    .setUserCol('userId')\\\n",
    "#    .setItemCol('movieId')\\\n",
    "#    .setRatingCol('rating')\n",
    "\n",
    "# # now lets compute an evaluation metric for our test and validation dataset\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# # Create RMSE evaluator using the lable and predicted columns\n",
    "# # it will essentially calculate RMSE score based on these columns\n",
    "# reg_eval = RegressionEvaluator(PredictionCol='prediction',labelCol='rating',metricName='rmse')\n",
    "\n",
    "# tolerance = 0.03\n",
    "\n",
    "# # now to understand rank let us initially assume that my recommendation matrix is 1000 x 1000 (1000 users and 1000 products, this is a very sparse matrix)\n",
    "# # now we get 2 matrices P (shape 1000 * rank) and Q (shape rank * 1000) so if we multiply them we get the same matrix but our storage have gone down from storing (1000 x 1000) to (2 * 1000 * rank) (for rank = 4 we only need 8000 numbers compared to 1000000)\n",
    "\n",
    "# ranks = [4,8,12]\n",
    "# errors = [0,0,0]\n",
    "# models = [0,0,0]\n",
    "# err = 0\n",
    "# min_error = float('inf')\n",
    "# best_rank = 1\n",
    "# for rank in ranks:\n",
    "#     # Set the ranks here\n",
    "#     als.setRank(rank)\n",
    "#     # create the model with these parameters\n",
    "#     model = als.fit(training_df)\n",
    "#     # Run the model to create the prediction Predict against the validation_df\n",
    "#     predict_df = model.transform(validation_df)\n",
    "    \n",
    "#     # Remove NaN values from predictions (due to spark-14489)\n",
    "#     predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n",
    "    \n",
    "#     # Run the previously created RSME evaluator , reg_eval, on the predicted_ratings_df DataFrame\n",
    "#     error = reg_eval.evaluate(predicted_ratings_df)\n",
    "#     error[err] = error\n",
    "#     models[err] = error\n",
    "#     print 'For rank %s the RMSE is %s' % (rank,error)\n",
    "#     if error < min_error:\n",
    "#         min_error = error\n",
    "#         best_rank = err\n",
    "#     err += 1\n",
    "    \n",
    "# als.setRank(ranks[best_rank])\n",
    "# print 'The best model was trained with rank %s' % ranks[best_rank]\n",
    "# my_model = models[best_rank]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d321553-d7d5-4a1b-bf2f-187b2961db89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS()\n",
    "\n",
    "# reset the parameters from the als object\n",
    "als.setPredictionCol('prediction')\\\n",
    "   .setMaxIter(5)\\\n",
    "   .setSeed(seed)\\\n",
    "   .setRegParam(0.1)\\\n",
    "   .setUserCol('userId')\\\n",
    "   .setItemCol('movieId')\\\n",
    "   .setRatingCol('rating')\\\n",
    "   .setRank(8)              # we got rank 8 as optimal\n",
    "\n",
    "# creating model with these parameters\n",
    "my_rating_model = als.fit(training_df)   # now this is our movie rating model which we will use for recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62ac3753-5a7c-4c18-b939-faf6d78d5fdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Looking for RMSE again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bab29f1b-7241-4e4b-9242-030a23ea6f5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has an RMSE on the test set of 0.813771546438166\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "# create a RMSE evaluator using the lable and predicted columns\n",
    "# it will essentially calculate the RMSE score based on these columns\n",
    "reg_eval = RegressionEvaluator(predictionCol='prediction',labelCol='rating',metricName='rmse')\n",
    "my_predict_df = my_rating_model.transform(test_df)\n",
    "\n",
    "# remove NaN values from prediction\n",
    "predicted_test_my_ratings_df = my_predict_df.filter(my_predict_df.prediction != float('nan'))\n",
    "\n",
    "#Run the previously created RMSE evaluator , reg_eval, on the predicted_test_my_ratings_df Dataframe\n",
    "test_RMSE_my_ratings = reg_eval.evaluate(predicted_test_my_ratings_df)\n",
    "print('The model has an RMSE on the test set of {0}'.format(test_RMSE_my_ratings))\n",
    "dbutils.widgets.text(\"input\",\"5\",\"\")\n",
    "ins=dbutils.widgets.get(\"input\")\n",
    "uid=int(ins)\n",
    "ll=predicted_test_my_ratings_df.filter(col('userId')==uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6692cffc-d304-49c5-998a-bd9653a2ddff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To get output from the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b3bd99-ace3-480a-92ee-bc9893a83514",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "[Row(title='Toy Story (1995)'), Row(title='Dead Man Walking (1995)'), Row(title='Clueless (1995)'), Row(title='Before and After (1996)'), Row(title='Batman Forever (1995)'), Row(title='Hackers (1995)'), Row(title='Boys on the Side (1995)'), Row(title='Natural Born Killers (1994)'), Row(title='Outbreak (1995)'), Row(title='Quiz Show (1994)')]"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "[Row(title='Toy Story (1995)'), Row(title='Dead Man Walking (1995)'), Row(title='Clueless (1995)'), Row(title='Before and After (1996)'), Row(title='Batman Forever (1995)'), Row(title='Hackers (1995)'), Row(title='Boys on the Side (1995)'), Row(title='Natural Born Killers (1994)'), Row(title='Outbreak (1995)'), Row(title='Quiz Show (1994)')]",
       "metadata": {},
       "type": "exit"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MovieRec=ll.join(movies_df,F.col('movieID') == F.col('ID')).drop('ID').select('title').take(10)\n",
    "\n",
    "l=dbutils.notebook.exit(MovieRec)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3063013607618963,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Movie_Lens_20M",
   "notebookOrigID": 2556915807025807,
   "widgets": {
    "input": {
     "currentValue": "5",
     "nuid": "b7ce2b99-3de1-40f0-860a-ad1aad917893",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "5",
      "label": "",
      "name": "input",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
